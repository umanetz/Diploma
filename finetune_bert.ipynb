{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\tvocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls ./files/bert/rubert_cased_L-12_H-768_A-12_pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0509 14:43:03.467619 140446016481088 file_utils.py:41] PyTorch version 1.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предобученный Берт для русского языка (iPavlov)\n",
    "BERT_PATH = './files/bert/rubert_cased_L-12_H-768_A-12_pt/'\n",
    "DATA_PATH = './files/data/data_bert/'\n",
    "FINETUNE_MODEL_PATH = './files/bert/rubert_finetune/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 102\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, target_mapping):\n",
    "        \"\"\"\n",
    "            Extract sample\n",
    "\n",
    "            ----------\n",
    "            data : array-like of shape (n_samples, )\n",
    "                Every sample is the dictionary with keys ['target','sent','data']. Where data is parameters of one sample after bert tokenization.\n",
    "            target_mapping: dict\n",
    "                Number associated with real target. Keys range from 0 to number of classes\n",
    "        \"\"\"        \n",
    "        self.sentences_features = data\n",
    "        self.target_mapping = target_mapping\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.sentences_features[idx]['data']\n",
    "        sample['target'] = self.target_mapping[self.sentences_features[idx]['target']]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(DATA_PATH + 'train.npy')[:]\n",
    "test_data = np.load(DATA_PATH + 'test.npy')[:]\n",
    "dev_data = np.load(DATA_PATH + 'dev.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к классы идут, не по-порядку присвоим им последовательные значения от 0 до кол-ва классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во классов 80\n"
     ]
    }
   ],
   "source": [
    "target_mapping = pd.value_counts([x['target'] for x in train_data])\n",
    "target_mapping = dict(zip(target_mapping.index, range(len(target_mapping))))\n",
    "num_classes = len(target_mapping)\n",
    "print('Кол-во классов', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [target_mapping[x['target']] for x in train_data]\n",
    "weight = pd.value_counts(targets).sort_index() / sum(pd.value_counts(targets))\n",
    "weight = torch.tensor(list(weight.values[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TextDataset(train_data, target_mapping)\n",
    "testset = TextDataset(test_data, target_mapping)\n",
    "devset = TextDataset(dev_data, target_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size)\n",
    "dev_loader = DataLoader(devset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0509 14:43:29.877933 140446016481088 configuration_utils.py:280] loading configuration file ./files/bert/rubert_cased_L-12_H-768_A-12_pt/config.json\n",
      "I0509 14:43:29.878886 140446016481088 configuration_utils.py:318] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "I0509 14:43:29.879456 140446016481088 modeling_utils.py:505] loading weights file ./files/bert/rubert_cased_L-12_H-768_A-12_pt/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "initial_model = BertModel.from_pretrained(pretrained_model_name_or_path = BERT_PATH, cache_dir=None).to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneBert(nn.Module):\n",
    "    def __init__(self, initial_model, output_dim, freeze_layers):\n",
    "        super(FinetuneBert, self).__init__()\n",
    "        self.bert = initial_model\n",
    "        self.cls = nn.Linear(768, output_dim)\n",
    "        for layer_idx in freeze_layers:\n",
    "            print (\"Froze Layer: \", layer_idx)\n",
    "            for param in list(self.bert.encoder.layer[layer_idx].parameters()):\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input, attention_mask):\n",
    "        _, x = self.bert(input, attention_mask=attention_mask)\n",
    "        return self.cls(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze Layer:  0\n",
      "Froze Layer:  1\n",
      "Froze Layer:  2\n",
      "Froze Layer:  3\n",
      "Froze Layer:  4\n",
      "Froze Layer:  5\n"
     ]
    }
   ],
   "source": [
    "#обучаем последние 12 - freeze_layers_num слоев\n",
    "freeze_layers_num = 6\n",
    "\n",
    "for m in initial_model.parameters():\n",
    "    m.requires_grad = True\n",
    "    \n",
    "model = FinetuneBert(initial_model, num_classes, range(freeze_layers_num)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([{'params': model.bert.parameters(), 'lr': 2e-5, 'name':'Bert'}, \n",
    "                        {'params': model.cls.parameters(), 'name':'Cls'}], lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight.to(device))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def f1_metrics(pred, true):\n",
    "    pred = pred.argmax(1).data.numpy()\n",
    "    true = true.data.numpy()\n",
    "    return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    print('LR: ', end='')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f'{param_group[\"name\"]}: {param_group[\"lr\"]},', end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader):\n",
    "    metrics = {'loss': 0, 'f1': 0}\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader, desc='train...'):\n",
    "        batch = {t: batch[t].to(device) for t in batch}\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = criterion(output, batch['target']) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        f1 = f1_metrics(output.cpu(), batch['target'].cpu())\n",
    "        for n, m in zip(['loss','f1'], [loss.item(), f1]):\n",
    "            metrics[n] =  metrics[n] + m\n",
    "    return {n: l / len(dataloader) for n, l in metrics.items()}\n",
    "\n",
    "def test(model, criterion, dataloader):\n",
    "    metrics = {'loss': 0, 'f1': 0}\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader,  desc='test...'):\n",
    "        batch = {t: batch[t].to(device) for t in batch}\n",
    "        output = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = criterion(output, batch['target']) \n",
    "        f1 = f1_metrics(output.cpu(), batch['target'].cpu())\n",
    "        for n, m in zip(['loss','f1'], [loss.item(), f1]):\n",
    "            metrics[n] =  metrics[n] + m\n",
    "    return {n: l / len(dataloader) for n, l in metrics.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_metrics(metrics_train, metrics_test):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.grid()\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.grid()\n",
    "    plt.plot([x['loss'] for x in metrics_test], label='test')\n",
    "    plt.plot([x['loss'] for x in metrics_train], label='train')\n",
    "    plt.legend()\n",
    "    plt.title('LOSS')\n",
    "\n",
    "    plt.subplot(2, 2,  2)\n",
    "    plt.grid()\n",
    "    plt.plot([x['f1'] for x in metrics_test], label='test')\n",
    "    plt.plot([x['f1'] for x in metrics_train], label='train')\n",
    "    plt.legend()\n",
    "    plt.title('F1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_metics = train(model, optimizer, criterion, train_loader)\n",
    "    train_loss.append(train_metics)\n",
    "    \n",
    "    test_metics = test(model, criterion, test_loader)\n",
    "    test_loss.append(test_metics)\n",
    "    clear_output(wait=False)\n",
    "    print(f\"Epoch: {epoch+1:02}, Train Loss: {train_loss[-1]['loss']:.3f}, Train F1: {train_loss[-1]['f1']:.3f},  Test Loss: {test_loss[-1]['loss']:.3f}, Test F1: {test_loss[-1]['f1']:.3f}\")\n",
    "    get_lr(optimizer)\n",
    "    scheduler.step()\n",
    "          \n",
    "    torch.save(model.state_dict(), FINETUNE_MODEL_PATH + 'current_model.pt')      \n",
    "    if best_f1 < test_loss[-1]['f1']:\n",
    "        torch.save(model.state_dict(), FINETUNE_MODEL_PATH + 'best_model.pt')\n",
    "        best_f1 = test_loss[-1]['f1']\n",
    "    plot_metrics(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('files/bert/rubert_finetune/best_model.pt', map_location=torch.device('cpu')))\n",
    "# model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
